# =============================================================================
# AGENT CONTAINER (V1.1) - Multi-LLM Support
# =============================================================================
# A single Claude Code agent worker with multi-LLM provider support.
#
# Responsibilities:
# - Listen for tasks on Redis
# - Run LLM sessions (Claude Code, GPT, Grok, Ollama, etc.)
# - Report status via heartbeat
# - Publish results back to orchestrator
#
# Providers:
# - claude-code: Claude Code SDK (agentic coding with tool use)
# - claude: Claude via llm CLI
# - openai: OpenAI GPT models via llm CLI
# - grok: xAI Grok via llm CLI
# - ollama: Local models via Ollama
# - openrouter: Multiple providers via OpenRouter
#
# Each container runs ONE agent. Scale with docker-compose --scale agent=N
# Configure provider with AGENT_PROVIDER env var (default: claude-code)
# =============================================================================

FROM oven/bun:1-alpine

WORKDIR /app

# Install system dependencies
# - git: For worktree operations
# - curl: Health checks
# - nodejs + npm: Required by Claude Code SDK
# - python3 + pip: Required by llm CLI
RUN apk add --no-cache \
    git \
    curl \
    nodejs \
    npm \
    openssh-client \
    python3 \
    py3-pip

# Install Claude Code CLI globally
RUN npm install -g @anthropic-ai/claude-code

# Install llm CLI and plugins for multi-LLM support
# Using --break-system-packages since we're in a container
RUN pip3 install --break-system-packages llm && \
    pip3 install --break-system-packages llm-anthropic llm-ollama || true

# Copy V1.1 package files
COPY V1.1/package.json V1.1/bun.lock* ./

# Install dependencies
RUN bun install --frozen-lockfile || bun install

# Copy V1.1 agent code
COPY V1.1/agent/ ./agent/
COPY V1.1/redis/ ./redis/
COPY V1.1/src/ ./src/
COPY V1.1/tsconfig.json ./

# Create directories
RUN mkdir -p /worktrees /.archon

# No exposed ports - agent communicates via Redis only

# Start agent worker
CMD ["bun", "run", "agent/worker.ts"]
